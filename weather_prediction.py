# -*- coding: utf-8 -*-
"""Weather_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wq-63NRnifnd1LzYz3wPuNmj-k_zAOjk
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.feature_selection import SelectKBest
from sklearn.ensemble import RandomForestRegressor
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

"""#Load Dataset"""

df = pd.read_csv("/content/weather_data.csv")
df

"""#Pre-Processing Data"""

print(df.shape)
print(df.columns)
print(df.info())
print(df.isnull().sum())

#Removing unnecessary columns

df = df.drop(columns=["LandAvgTempUncertainty", "LandMaxTempUncertainty",
                          "LandMinTempUncertainty", "Land_OceanAvgTempUncertainty"], axis=1)

#Converting the date column to datetime object
df["dt"] = pd.to_datetime(df["dt"])
df["Month"] = df["dt"].dt.month
df["Year"] = df["dt"].dt.year

#Removing month and dt column since we are predicting data based on annual record

df = df.drop("dt", axis=1)
df = df.drop("Month", axis=1)
df = df[df.Year >= 1850]
df = df.set_index(["Year"])

#removing rows with null values

df = df.dropna()

"""# Data Visualization"""

#Correlation Matrix

corrMatrix = df.corr()
sns.heatmap(corrMatrix, annot=True)
plt.show()

#Graph visualization

plt.scatter(x=df["LandAvgTemp"], y=df["Land_OceanAvgTemp"])
plt.show()

plt.scatter(x=df["LandMaxTemp"], y=df["Land_OceanAvgTemp"])
plt.show()

plt.scatter(x=df["LandMinTemp"], y=df["Land_OceanAvgTemp"])
plt.show()

#Splitting dependent and independent variable

y = df["Land_OceanAvgTemp"]
x = df[["LandAvgTemp", "LandMaxTemp","LandMinTemp"]]

#train test split for training the model

xtrain, x_test, ytrain, y_test = train_test_split(x, y, test_size=0.25, random_state=42)

#data

print(xtrain.shape)
print(x_test.shape)
print(ytrain.shape)
print(y_test.shape)

#Training the model

model = make_pipeline(
          SelectKBest(k="all"),
          StandardScaler(),
          RandomForestRegressor(n_estimators=100, max_depth=50, random_state=77, n_jobs=-1)
        )

model.fit(xtrain, ytrain)

y_pred = model.predict(x_test)

#check error, print mean square error(MSE) and mean absolute percentage error(mape)

errors = abs(y_pred - y_test)
print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')
mape = mape = 100 * (errors / y_test)

accuracy = 100 - np.mean(mape)
print('Accuracy:', round(accuracy, 2), '%.')

model.predict([[0.749,8.242,-3.206]])